<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0">
		<title>LLMs represent a win for utility theory</title>
		<meta name="description" content="AI&#39;s success shows we don&#39;t need to encode heuristics and biases into agent based models">
		<link rel="alternate" href="/feed/feed.xml" type="application/atom+xml" title="Processing Beliefs">
		<link rel="alternate" href="/feed/feed.json" type="application/json" title="Processing Beliefs">

		<!-- Google tag (gtag.js) -->
		<script async src="https://www.googletagmanager.com/gtag/js?id=G-YSZ8X5QRT0"></script>
		<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'G-YSZ8X5QRT0');
		</script>
		
		<style>/**
 * okaidia theme for JavaScript, CSS and HTML
 * Loosely based on Monokai textmate theme by http://www.monokai.nl/
 * @author ocodia
 */

code[class*="language-"],
pre[class*="language-"] {
	color: #f8f8f2;
	background: none;
	text-shadow: 0 1px rgba(0, 0, 0, 0.3);
	font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
	font-size: 1em;
	text-align: left;
	white-space: pre;
	word-spacing: normal;
	word-break: normal;
	word-wrap: normal;
	line-height: 1.5;

	-moz-tab-size: 4;
	-o-tab-size: 4;
	tab-size: 4;

	-webkit-hyphens: none;
	-moz-hyphens: none;
	-ms-hyphens: none;
	hyphens: none;
}

/* Code blocks */
pre[class*="language-"] {
	padding: 1em;
	margin: .5em 0;
	overflow: auto;
	border-radius: 0.3em;
}

:not(pre) > code[class*="language-"],
pre[class*="language-"] {
	background: #272822;
}

/* Inline code */
:not(pre) > code[class*="language-"] {
	padding: .1em;
	border-radius: .3em;
	white-space: normal;
}

.token.comment,
.token.prolog,
.token.doctype,
.token.cdata {
	color: #8292a2;
}

.token.punctuation {
	color: #f8f8f2;
}

.token.namespace {
	opacity: .7;
}

.token.property,
.token.tag,
.token.constant,
.token.symbol,
.token.deleted {
	color: #f92672;
}

.token.boolean,
.token.number {
	color: #ae81ff;
}

.token.selector,
.token.attr-name,
.token.string,
.token.char,
.token.builtin,
.token.inserted {
	color: #a6e22e;
}

.token.operator,
.token.entity,
.token.url,
.language-css .token.string,
.style .token.string,
.token.variable {
	color: #f8f8f2;
}

.token.atrule,
.token.attr-value,
.token.function,
.token.class-name {
	color: #e6db74;
}

.token.keyword {
	color: #66d9ef;
}

.token.regex,
.token.important {
	color: #fd971f;
}

.token.important,
.token.bold {
	font-weight: bold;
}
.token.italic {
	font-style: italic;
}

.token.entity {
	cursor: help;
}
/*
 * New diff- syntax
 */

pre[class*="language-diff-"] {
	--eleventy-code-padding: 1.25em;
	padding-left: var(--eleventy-code-padding);
	padding-right: var(--eleventy-code-padding);
}
.token.deleted {
	background-color: hsl(0, 51%, 37%);
	color: inherit;
}
.token.inserted {
	background-color: hsl(126, 31%, 39%);
	color: inherit;
}

/* Make the + and - characters unselectable for copy/paste */
.token.prefix.unchanged,
.token.prefix.inserted,
.token.prefix.deleted {
	-webkit-user-select: none;
	user-select: none;
	display: inline-flex;
	align-items: center;
	justify-content: center;
	padding-top: 2px;
	padding-bottom: 2px;
}
.token.prefix.inserted,
.token.prefix.deleted {
	width: var(--eleventy-code-padding);
	background-color: rgba(0,0,0,.2);
}

/* Optional: full-width background color */
.token.inserted:not(.prefix),
.token.deleted:not(.prefix) {
	display: block;
	margin-left: calc(-1 * var(--eleventy-code-padding));
	margin-right: calc(-1 * var(--eleventy-code-padding));
	text-decoration: none; /* override del, ins, mark defaults */
	color: inherit; /* override del, ins, mark defaults */
}
.post-metadata {
    list-style: none;
    padding: 0;
    margin: 1rem 0;
    display: flex;
    flex-wrap: wrap;
    gap: 0.5rem;
    align-items: center;
}

.post-metadata li {
    margin: 0;
}

.post-tag {
    display: inline-block;
    font-size: 0.875rem;
    background: #e5e5e5;
    padding: 0.25rem 0.75rem;
    border-radius: 0.25rem;
    text-decoration: none;
    color: inherit;
    transition: background-color 0.2s ease;
}

.post-tag:hover {
    background: #d5d5d5;
    text-decoration: none;
}

/* Table Improvements */
table {
    width: 100%;
    border-collapse: collapse;
    margin: 1.5em 0;
    font-size: 0.95em;
    box-shadow: 0 1px 3px rgba(0,0,0,0.1);
    border-radius: 6px;
    overflow: hidden;
}
* { box-sizing: border-box; }
/* Defaults */
:root {
	--font-family: -apple-system, system-ui, sans-serif;
	--font-family-monospace: Consolas, Menlo, Monaco, Andale Mono WT, Andale Mono, Lucida Console, Lucida Sans Typewriter, DejaVu Sans Mono, Bitstream Vera Sans Mono, Liberation Mono, Nimbus Mono L, Courier New, Courier, monospace;
}

/* Theme colors */
:root {
	--color-gray-20: #e0e0e0;
	--color-gray-50: #C0C0C0;
	--color-gray-90: #333;

	--background-color: #fff;

	--text-color: var(--color-gray-90);
	--text-color-link: #082840;
	--text-color-link-active: #5f2b48;
	--text-color-link-visited: #17050F;

	--syntax-tab-size: 2;
}

@media (prefers-color-scheme: dark) {
	:root {
		--color-gray-20: #e0e0e0;
		--color-gray-50: #C0C0C0;
		--color-gray-90: #dad8d8;

		/* --text-color is assigned to --color-gray-_ above */
		--text-color-link: #1493fb;
		--text-color-link-active: #6969f7;
		--text-color-link-visited: #a6a6f8;

		--background-color: #15202b;
	}
}


/* Global stylesheet */
* {
	box-sizing: border-box;
}

html,
body {
	padding: 0;
	margin: 0 auto;
	font-family: var(--font-family);
	color: var(--text-color);
	background-color: var(--background-color);
}
html {
	overflow-y: scroll;
}
body {
	max-width: 40em;
}

/* https://www.a11yproject.com/posts/how-to-hide-content/ */
.visually-hidden {
	clip: rect(0 0 0 0);
	clip-path: inset(50%);
	height: 1px;
	overflow: hidden;
	position: absolute;
	white-space: nowrap;
	width: 1px;
}

p:last-child {
	margin-bottom: 0;
}
p {
	line-height: 1.5;
}

li {
	line-height: 1.5;
}

a[href] {
	color: var(--text-color-link);
}
a[href]:visited {
	color: var(--text-color-link-visited);
}
a[href]:hover,
a[href]:active {
	color: var(--text-color-link-active);
}

main {
	padding: 1rem;
}
main :first-child {
	margin-top: 0;
}

header {
	border-bottom: 1px dashed var(--color-gray-20);
}
header:after {
	content: "";
	display: table;
	clear: both;
}

.links-nextprev {
	list-style: none;
	border-top: 1px dashed var(--color-gray-20);
	padding: 1em 0;
}

table {
	margin: 1em 0;
}
table td,
table th {
	padding-right: 1em;
}

pre,
code {
	font-family: var(--font-family-monospace);
}
pre:not([class*="language-"]) {
	margin: .5em 0;
	line-height: 1.375; /* 22px /16 */
	-moz-tab-size: var(--syntax-tab-size);
	-o-tab-size: var(--syntax-tab-size);
	tab-size: var(--syntax-tab-size);
	-webkit-hyphens: none;
	-ms-hyphens: none;
	hyphens: none;
	direction: ltr;
	text-align: left;
	white-space: pre;
	word-spacing: normal;
	word-break: normal;
}
code {
	word-break: break-all;
}

/* Header */
header {
	display: flex;
	gap: 1em .5em;
	flex-wrap: wrap;
	align-items: center;
	padding: 1em;
}
.home-link {
	font-size: 1em; /* 16px /16 */
	font-weight: 700;
	margin-right: 2em;
}
.home-link:link:not(:hover) {
	text-decoration: none;
}

/* Nav */
.nav {
	display: flex;
	padding: 0;
	margin: 0;
	list-style: none;
}
.nav-item {
	display: inline-block;
	margin-right: 1em;
}
.nav-item a[href]:not(:hover) {
	text-decoration: none;
}
.nav a[href][aria-current="page"] {
	text-decoration: underline;
}

/* Posts list */
.postlist {
	list-style: none;
	padding: 0;
	padding-left: 1.5rem;
}
.postlist-item {
	display: flex;
	flex-wrap: wrap;
	align-items: baseline;
	counter-increment: start-from -1;
	margin-bottom: 1em;
}
.postlist-item:before {
	display: inline-block;
	pointer-events: none;
	content: "" counter(start-from, decimal-leading-zero) ". ";
	line-height: 100%;
	text-align: right;
	margin-left: -1.5rem;
}
.postlist-date,
.postlist-item:before {
	font-size: 0.8125em; /* 13px /16 */
	color: var(--color-gray-90);
}
.postlist-date {
	word-spacing: -0.5px;
}
.postlist-link {
	font-size: 1.1875em; /* 19px /16 */
	font-weight: 700;
	flex-basis: calc(100% - 1.5rem);
	padding-left: .25em;
	padding-right: .5em;
	text-underline-position: from-font;
	text-underline-offset: 0;
	text-decoration-thickness: 1px;
}
.postlist-item-active .postlist-link {
	font-weight: bold;
}

/* Tags */
.post-tag {
	display: inline-flex;
	align-items: center;
	justify-content: center;
	text-transform: capitalize;
	font-style: italic;
}
.postlist-item > .post-tag {
	align-self: center;
}

/* Tags list */
.post-metadata {
	display: inline-flex;
	flex-wrap: wrap;
	gap: .5em;
	list-style: none;
	padding: 0;
	margin: 0;
}
.post-metadata time {
	margin-right: 1em;
}

/* Direct Links / Markdown Headers */
.header-anchor {
	text-decoration: none;
	font-style: normal;
	font-size: 1em;
	margin-left: .1em;
}
a[href].header-anchor,
a[href].header-anchor:visited {
	color: transparent;
}
a[href].header-anchor:focus,
a[href].header-anchor:hover {
	text-decoration: underline;
}
a[href].header-anchor:focus,
:hover > a[href].header-anchor {
	color: #aaa;
}

h2 + .header-anchor {
	font-size: 1.5em;
}

/* Table Improvements */
table {
    width: 100%;
    border-collapse: collapse;
    margin: 1.5em 0;
    font-size: 0.95em;
    box-shadow: 0 1px 3px rgba(0,0,0,0.1);
    border-radius: 6px;
    overflow: hidden;
}

table th {
    background-color: #f2f2f2;
    font-weight: 600;
    text-align: left;
    padding: 0.75em 1em;
    border-bottom: 2px solid #ddd;
}

table td {
    padding: 0.75em 1em;
    border-bottom: 1px solid #eee;
}

table tr:last-child td {
    border-bottom: none;
}

table tr:nth-child(even) {
    background-color: #f9f9f9;
}

table input[type="number"] {
    width: 100%;
    padding: 0.5em;
    border: 1px solid #ddd;
    border-radius: 4px;
    font-size: 0.9em;
}

/* Highlighted row styling */
table tr[style*="font-weight: bold"] {
    background-color: #f0f7ff;
}

/* Dark mode adjustments for tables */
@media (prefers-color-scheme: dark) {
    table th {
        background-color: #2a2a2a;
        border-bottom: 2px solid #444;
    }
    
    table td {
        border-bottom: 1px solid #333;
    }
    
    table tr:nth-child(even) {
        background-color: #1a2632;
    }
    
    table tr[style*="font-weight: bold"] {
        background-color: #1c2d3f;
    }
    
    table input[type="number"] {
        background-color: #2a2a2a;
        border-color: #444;
        color: var(--color-gray-90);
    }
}</style>
	</head>
	<body>
		<a href="#skip" class="visually-hidden">Skip to main content</a>

		<header>
			<a href="/" class="home-link">Processing Beliefs</a>
			<nav>
				<h2 class="visually-hidden">Top level navigation menu</h2>
				<ul class="nav">
					<li class="nav-item"><a href="/">Home</a></li>
					<li class="nav-item"><a href="/blog/">Archive</a></li>
					<li class="nav-item"><a href="/about/">About</a></li>
				</ul>
			</nav>
		</header>

		<main id="skip">
			

<h1>LLMs represent a win for utility theory</h1>

<ul class="post-metadata">
	<li><time datetime="2025-06-05">05 June 2025</time></li>
	<li><a href="/tags/complexity/" class="post-tag">complexity</a></li>
	<li><a href="/tags/llms/" class="post-tag">LLMs</a></li>
</ul>


<p>Our political-economic system emerges from complex interactions among humans and firms. Since the 1970s, economists have mostly agreed that we cannot understand the system as a whole without some model of the individuals within that system. Both neoclassical and behavioral economists model these individuals as if they maximize a utility function. <a href="https://yalebooks.yale.edu/book/9780300283327/making-sense-of-chaos/">Complexity economists</a> and <a href="https://global.oup.com/academic/product/simple-heuristics-that-make-us-smart-9780195143812">psychologists</a> argue that instead we should model humans as following simple rules, or heuristics. They note that any realistic utility function would be extremely complex, context dependent and different across individuals, making the whole concept impractical. But the success of LLMs, like ChatGPT, shows that systems can, in fact, learn complicated, context-dependent utility functions that are tailorable to individual preferences, thereby supporting the utility function view. The rest of this post shows how utility maximization drives the LLMs we use today.</p>
<p>LLMs go through various training phases, the final of which, post-training, allows us to ask the model simple questions and receive natural-sounding responses. One post-training step, reinforcement learning from human feedback (RLHF), or with AI feedback (RLAIF), seeks to encode human stylistic and content preferences into the LLM.[^RLHF_BOOK] RLHF works by feeding a prompt to a language model and then having it output several different completions to the prompt. A labeler then notes which completion it prefers. With this labeled dataset, another LLM then learns to predict the labelers' preferences over the completions. Using reinforcement learning with the reward model, the base LLM learns to fine tune its responses.<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup></p>
<p>Economists follow a similar formula when they try to estimate utility functions. They observe consumers choosing between various affordable items and then note which was purchased, or preferred. From data on many such transactions economists then fit a model, much smaller than an LLM, to estimate a utility function. Although focusing on surveys rather than behavior, Daniel Kahneman and Amos Tversky's experiments asked subjects questions such as whether they would prefer a coin flip that paid $120 for heads and nothing for tails versus a guaranteed $50. From these surveys they estimated utility functions that showed humans take larger risks to avoid losses than they would to make a similar-sized gain. This research won the Nobel prize.</p>
<p>Note the similarity between RLHF and Kahneman and Tversky's experiments: both methods first ask a human responder to label which of two outcomes they prefer, then estimate a utility function from this data. <a href="https://arxiv.org/abs/2402.01306">Ethayarajh et al</a> dig into this connection, arguing that the best performing models implicitly use utility functions similar to those proposed by Kahneman and Tversky. They propose reward models directly based on Kahneman and Tversky-inspired utility functions.</p>
<p>Reward models show we can learn utility functions, but what about the problem of them changing across contexts and people? LLM-based reward models easily deal with even slight contextual shifts. During pretraining, the model learns that wine relates more closely with vineyard-related vocabulary than beer does. RLHF reinforces these relationships, as the model learns how the context of a prompt impacts the completions that labelers prefer.</p>
<p>Incorporating diverse human preferences presents a larger challenge. Training on just one reward model assumes we all have the same preferences. But even data labelers have different preferences, with different groups producing reward models with different predictions. Taking advantage of this variation in reward models, researchers recently released an <a href="https://arxiv.org/abs/2409.20296">open source dataset</a> to build and evaluate new methods to build personalized LLMs. Companies like Netflix, Meta, and Amazon profited massively from personalized content. Doing the same with LLMs will likely drive similarly massive profits. Therefore, this problem is unlikely to persist.</p>
<p>RLHF's success shows that we can use data to estimate flexible utility functions that account for diverse contexts and preferences. Therefore, economists and others modeling complex systems can avoid encoding long lists of heuristics into their agents' behavior. Instead, they can supply data from which agents learn utility functions. Simulations of these more realistic agents interacting will teach us new lessons about how these complex adaptive systems evolve.</p>
<h2 id="footnotes" tabindex="-1">Footnotes <a class="header-anchor" href="#footnotes">#</a></h2>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>Direct policy optimization, part of the broader family of direct alignment algorithms, doesn't explicitly train a reward model or use reinforcement learning to update the LLM. But these are still done implicitly. The models still optimize the Bradley-Terry preference model. <a href="#fnref1" class="footnote-backref">↩︎</a></p>
</li>
</ol>
</section>


<script src="https://utteranc.es/client.js"
		repo="tristinb/tristinb.github.io"
		issue-term="title"
		label="💬"
		theme="preferred-color-scheme"
		crossorigin="anonymous"
		async>
</script>

<script src="/bundle.js"></script>
<ul class="links-nextprev"><li>Previous: <a href="/blog/innovation_collective_intelligence/">Innovation, knowledge workers and collective intelligence</a></li>
</ul>
		</main>

		<footer></footer>

		<!-- Current page: /blog/llm_utility_functions/ -->
	</body>
</html>
